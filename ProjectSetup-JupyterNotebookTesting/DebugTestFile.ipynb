{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BoxOfficeMojo Python WebScraper Utility Package\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def downloadhtml(url):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        if 'Sorry, we\\'re not able to process your request.' in BeautifulSoup(page.content, 'lxml').find('div', id='main').find('p'):\n",
    "            return None\n",
    "        else:\n",
    "            soup = BeautifulSoup(page.content, 'lxml')\n",
    "            return soup\n",
    "    except TimeoutError:\n",
    "        try:\n",
    "            page = requests.get(url)\n",
    "            if 'Sorry, we\\'re not able to process your request.' in BeautifulSoup(page.content, 'lxml').find('div', id='main').find('p'):\n",
    "                return None\n",
    "            else:\n",
    "                soup = BeautifulSoup(page.content, 'lxml')\n",
    "                return soup\n",
    "        except TimeoutError:\n",
    "            return 'TimeoutError'\n",
    "\n",
    "\n",
    "def uppertabledataparser(soup):\n",
    "    upper_table_data_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find('table', bgcolor='#dcdcdc').find_all('tr')\n",
    "    return upper_table_data_rows\n",
    "\n",
    "\n",
    "def totallifetimegrossestableparser(soup):\n",
    "    total_lifetime_grosses_table = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('table').find_all('td')\n",
    "    return total_lifetime_grosses_table\n",
    "\n",
    "\n",
    "def titleparser(url):\n",
    "    try:\n",
    "        soup = downloadhtml(url)\n",
    "        if soup is None:\n",
    "            return 'PageMissing' + ' ' + str(url)\n",
    "        elif soup == 'TimeoutError':\n",
    "            return 'TimeoutError' + ' ' + str(url)\n",
    "        else:\n",
    "            title = soup.find('div', id='body').find('table', style='padding-top: 5px;').find('b').text\n",
    "            return title\n",
    "    except AttributeError:\n",
    "        while True:\n",
    "            try:    \n",
    "                soup = downloadhtml(url)\n",
    "                if soup is None:\n",
    "                    return 'PageMissing' + ' ' + str(url)\n",
    "                elif soup == 'TimeoutError':\n",
    "                    return 'TimeoutError' + ' ' + str(url)\n",
    "                else:\n",
    "                    title = soup.find('div', id='body').find('table', style='padding-top: 5px;').find('b').text\n",
    "                return title\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            break        \n",
    "\n",
    "\n",
    "def distributorparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        upper_table_data_rows = uppertabledataparser(soup)\n",
    "        li = []\n",
    "        for tr in upper_table_data_rows:\n",
    "            td = tr.find_all('td')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "        for i in upper_table_dict.keys():\n",
    "            upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "        return upper_table_dict['Distributor']\n",
    "\n",
    "\n",
    "def domestictotalgrossparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        upper_table_data_rows = uppertabledataparser(soup)\n",
    "        li = []\n",
    "        for tr in upper_table_data_rows:\n",
    "            td = tr.find_all('td')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "        for i in upper_table_dict.keys():\n",
    "            upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "        return upper_table_dict['Domestic Total Gross']\n",
    "\n",
    "\n",
    "def genreparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        upper_table_data_rows = uppertabledataparser(soup)\n",
    "        li = []\n",
    "        for tr in upper_table_data_rows:\n",
    "            td = tr.find_all('td')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "        for i in upper_table_dict.keys():\n",
    "            upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "        return upper_table_dict['Genre']\n",
    "\n",
    "\n",
    "def mpaaratingparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        upper_table_data_rows = uppertabledataparser(soup)\n",
    "        li = []\n",
    "        for tr in upper_table_data_rows:\n",
    "            td = tr.find_all('td')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "        for i in upper_table_dict.keys():\n",
    "            upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "        return upper_table_dict['MPAA Rating']\n",
    "\n",
    "\n",
    "def productionbudgetparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        upper_table_data_rows = uppertabledataparser(soup)\n",
    "        li = []\n",
    "        for tr in upper_table_data_rows:\n",
    "            td = tr.find_all('td')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "        for i in upper_table_dict.keys():\n",
    "            upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "        return upper_table_dict['Production Budget']\n",
    "\n",
    "\n",
    "def releasedateparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        upper_table_data_rows = uppertabledataparser(soup)\n",
    "        li = []\n",
    "        for tr in upper_table_data_rows:\n",
    "            td = tr.find_all('td')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "        for i in upper_table_dict.keys():\n",
    "            upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "        return upper_table_dict['Release Date']\n",
    "\n",
    "\n",
    "def runtimeparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        upper_table_data_rows = uppertabledataparser(soup)\n",
    "        li = []\n",
    "        for tr in upper_table_data_rows:\n",
    "            td = tr.find_all('td')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "        for i in upper_table_dict.keys():\n",
    "            upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "        return upper_table_dict['Runtime']\n",
    "\n",
    "\n",
    "def domesticparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        total_lifetime_grosses_table_rows = totallifetimegrossestableparser(soup)\n",
    "        li = []\n",
    "        for tr in total_lifetime_grosses_table_rows:\n",
    "            li.append(tr.text)\n",
    "        for x in range(0, len(li)):\n",
    "            li[x] = li[x].replace('\\xa0', '')\n",
    "            li[x] = li[x].replace('+', '')\n",
    "            li[x] = li[x].replace('=', '')\n",
    "            li[x] = li[x].replace(':', '')\n",
    "        li = [x for x in li if x]\n",
    "        return li[1]\n",
    "\n",
    "\n",
    "def foreignparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        total_lifetime_grosses_table_rows = totallifetimegrossestableparser(soup)\n",
    "        li = []\n",
    "        for tr in total_lifetime_grosses_table_rows:\n",
    "            li.append(tr.text)\n",
    "        if len(li) == 2:\n",
    "            return None\n",
    "        else:\n",
    "            for x in range(0, len(li)):\n",
    "                li[x] = li[x].replace('\\xa0', '')\n",
    "                li[x] = li[x].replace('+', '')\n",
    "                li[x] = li[x].replace('=', '')\n",
    "                li[x] = li[x].replace(':', '')\n",
    "            li = [x for x in li if x]\n",
    "            return li[4]\n",
    "\n",
    "\n",
    "def worldwideparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        total_lifetime_grosses_table_rows = totallifetimegrossestableparser(soup)\n",
    "        li = []\n",
    "        for tr in total_lifetime_grosses_table_rows:\n",
    "            li.append(tr.text)\n",
    "        if len(li) == 2:\n",
    "            return None\n",
    "        else:\n",
    "            for x in range(0, len(li)):\n",
    "                li[x] = li[x].replace('\\xa0', '')\n",
    "                li[x] = li[x].replace('+', '')\n",
    "                li[x] = li[x].replace('=', '')\n",
    "                li[x] = li[x].replace(':', '')\n",
    "            li = [x for x in li if x]\n",
    "            return li[7]\n",
    "\n",
    "\n",
    "\n",
    "def openingweekendgrossparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        domesticsummary_table_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('table').find_next('table').find_all('td')\n",
    "        li = []\n",
    "        for tr in domesticsummary_table_rows:\n",
    "            li.append(tr.text)\n",
    "        for x in range(0, len(li)):\n",
    "            li[x] = li[x].replace('\\xa0', '')\n",
    "            li[x] = li[x].replace('+', '')\n",
    "            li[x] = li[x].replace('=', '')\n",
    "            li[x] = li[x].replace(':', '')\n",
    "        if len(li) == 2:\n",
    "            li = [None, None]\n",
    "        return li[1]\n",
    "\n",
    "\n",
    "def openingweekendtheatersparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        domesticsummary_table_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('table').find_next('table').find_all('td')\n",
    "        li = []\n",
    "        for tr in domesticsummary_table_rows:\n",
    "            li.append(tr.text)\n",
    "        for x in range(0, len(li)):\n",
    "            li[x] = li[x].replace('\\xa0', '')\n",
    "            li[x] = li[x].replace('+', '')\n",
    "            li[x] = li[x].replace('=', '')\n",
    "            li[x] = li[x].replace(':', '')\n",
    "        if len(li) == 2:\n",
    "            k = [None, None]\n",
    "        else:\n",
    "            k = li[2].split(', ')\n",
    "        return k[1]\n",
    "\n",
    "\n",
    "def widestreleaseparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        widest_release_table_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('table').find_next('table').find_next('table').find_all('tr')\n",
    "        li = []\n",
    "        for tr in widest_release_table_rows:\n",
    "            td = tr.find_all('td')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        for x in range(0, len(li)):\n",
    "            li[x] = li[x].replace('\\xa0', ' ')\n",
    "            li[x] = li[x].lstrip()\n",
    "            li[x] = li[x].replace(':', '')\n",
    "        return li[1]\n",
    "\n",
    "\n",
    "def directorparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "        li = []\n",
    "        for tr in the_players_rows:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        for i in range(0, len(li)-1):\n",
    "            if '*' in li[i]:\n",
    "                del li[i]\n",
    "        for i in range(0, len(li)-1):\n",
    "            if 'Director' in li[i]:\n",
    "                director_index = i\n",
    "            elif 'Writer' in li[i]:\n",
    "                writer_index = i    \n",
    "        try:\n",
    "            director_index\n",
    "        except NameError:\n",
    "            director_index = None\n",
    "\n",
    "        try:\n",
    "            writer_index\n",
    "        except NameError:\n",
    "            writer_index = None\n",
    "\n",
    "        if director_index is None:\n",
    "            director_data = None\n",
    "        elif writer_index is None:\n",
    "            director_data = li[1]\n",
    "        elif len(li[director_index+1:writer_index]) == 1:\n",
    "            director_data = li[director_index+1]\n",
    "        else:\n",
    "            director_data = li[director_index+1:writer_index]\n",
    "\n",
    "        return str(director_data)\n",
    "\n",
    "    \n",
    "def writerparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "        li = []\n",
    "        for tr in the_players_rows:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        for i in range(0, len(li)-1):\n",
    "            if '*' in li[i]:\n",
    "                del li[i]\n",
    "        for i in range(0, len(li)-1):\n",
    "            if 'Writer' in li[i]:\n",
    "                writer_index = i\n",
    "            elif 'Actors' in li[i]:\n",
    "                actor_index = i\n",
    "\n",
    "        try:\n",
    "            writer_index\n",
    "        except NameError:\n",
    "            writer_index = None\n",
    "\n",
    "        try:\n",
    "            actor_index\n",
    "        except NameError:\n",
    "            actor_index = None\n",
    "\n",
    "        if writer_index is None:\n",
    "            writer_data = None\n",
    "        elif actor_index is None:\n",
    "            writer_data = li[4]\n",
    "        elif len(li[writer_index+1:actor_index]) == 1:\n",
    "            writer_data = li[writer_index+1]\n",
    "        else:\n",
    "            writer_data = li[writer_index+1:actor_index]\n",
    "        return str(writer_data)\n",
    "\n",
    "\n",
    "def actorparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "        li = []\n",
    "        for tr in the_players_rows:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        for i in range(0, len(li)-1):\n",
    "            if '*' in li[i]:\n",
    "                del li[i]\n",
    "        for i in range(0, len(li)-1):\n",
    "            if 'Actors' in li[i]:\n",
    "                actor_index = i\n",
    "            elif 'Producer' in li[i]:\n",
    "                producer_index = i\n",
    "        actors_data = li[actor_index+1:producer_index]\n",
    "        if len(li[actor_index+1:producer_index]) == 1:\n",
    "            actors_data = li[actor_index+1]\n",
    "        return str(actors_data)\n",
    "\n",
    "\n",
    "def producerparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "        li = []\n",
    "        for tr in the_players_rows:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        for i in range(0, len(li)-1):\n",
    "            if '*' in li[i]:\n",
    "                del li[i]\n",
    "        for i in range(0, len(li)-1):\n",
    "            if 'Producer' in li[i]:\n",
    "                producer_index = i\n",
    "            elif 'Composer' in li[i]:\n",
    "                composer_index = i\n",
    "        producer_data = li[producer_index+1:composer_index]\n",
    "        if len(li[producer_index+1:composer_index]) == 1:\n",
    "            producer_data = li[producer_index+1]\n",
    "        return str(producer_data)\n",
    "\n",
    "\n",
    "def composerparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "        li = []\n",
    "        for tr in the_players_rows:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        for i in range(0, len(li)-1):\n",
    "            if '*' in li[i]:\n",
    "                del li[i]\n",
    "        for i in range(0, len(li)-1):\n",
    "            if 'Composer' in li[i]:\n",
    "                composer_index = i\n",
    "        composer_data = li[composer_index+1:]\n",
    "        if len(composer_data) == 1:\n",
    "            composer_data = li[composer_index+1]\n",
    "        return str(composer_data)\n",
    "\n",
    "\n",
    "def genresparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        genres_table_rows = soup.find(\n",
    "            'div', id='body').find(\n",
    "            'table', style='padding-top: 5px;').find_next_sibling(\n",
    "            'table').find(\n",
    "            'table', width='100%').find(\n",
    "            'table', width='100%').find_next_sibling(\n",
    "            'div').find_all(\n",
    "            'a')\n",
    "        li = []\n",
    "        for tr in genres_table_rows:\n",
    "            td = tr.find_all('b')\n",
    "            for i in td:\n",
    "                li.append(i.text)\n",
    "        return str(li)\n",
    "\n",
    "\n",
    "def YearlyTop100sparser(url):\n",
    "    page = requests.get(url)\n",
    "    try:\n",
    "        soup = downloadhtml(url)\n",
    "        if soup is None:\n",
    "            return 'PageMissing' + ' ' + str(url)\n",
    "        elif soup == 'TimeoutError':\n",
    "            return 'TimeoutError' + ' ' + str(url)\n",
    "        else:\n",
    "            year = BeautifulSoup(page.content, 'lxml').find('div', id='body').find('table').find_next('table').find_next('table')\n",
    "            li = []\n",
    "            td = year.find_all('a')\n",
    "            for i in td:\n",
    "                li.append(i.get('href'))\n",
    "            YearlyTop100s = []\n",
    "            for i in range(0, len(li)-1):\n",
    "                if 'chart' in li[i]:\n",
    "                    YearlyTop100s.append(li[i])\n",
    "            for i in range(0, len(YearlyTop100s)):\n",
    "                YearlyTop100s[i] = 'https://www.boxofficemojo.com/yearly/'+ YearlyTop100s[i]\n",
    "            return YearlyTop100s\n",
    "    except AttributeError:\n",
    "        return 'AttributeError'\n",
    "\n",
    "def YearlyNonTop100sparser(url):\n",
    "    YearlyTop100s = YearlyTop100sparser(url)\n",
    "    YearlyNonTop100s = []\n",
    "    for x in YearlyTop100s:\n",
    "        page = requests.get(x)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        footer = soup.find('center')\n",
    "        footer_url_rows = footer.find_all('a')\n",
    "        for i in footer_url_rows:\n",
    "            YearlyNonTop100s.append(i.get('href'))\n",
    "    for i in range(0, len(YearlyNonTop100s)):\n",
    "        YearlyNonTop100s[i] = 'https://www.boxofficemojo.com'+ YearlyNonTop100s[i]\n",
    "    return YearlyNonTop100s\n",
    "\n",
    "\n",
    "def AllYearlyURLsparser(url):\n",
    "    AllYearlyURLs = []\n",
    "    YearlyTop100s = YearlyTop100sparser(url)\n",
    "    YearlyNonTop100s = YearlyNonTop100sparser(url)\n",
    "    for x in YearlyTop100s:\n",
    "        AllYearlyURLs.append(x)\n",
    "    for x in YearlyNonTop100s:\n",
    "        AllYearlyURLs.append(x)\n",
    "    return AllYearlyURLs\n",
    "\n",
    "\n",
    "def MasterURLsparser(url):\n",
    "    MasterURLs = []\n",
    "    AllYearlyURLs = AllYearlyURLsparser(url)\n",
    "    for x in AllYearlyURLs:\n",
    "        page = requests.get(x)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        body = soup.find('table', cellpadding='5')\n",
    "        urls_rows_ffffff = body.find_all('tr', bgcolor='#ffffff')\n",
    "        li = []\n",
    "        for tr in urls_rows_ffffff:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                li.append(i.get('href'))\n",
    "        urls_rows_f4f4ff = body.find_all('tr', bgcolor='#f4f4ff')\n",
    "        lis = []\n",
    "        for tr in urls_rows_f4f4ff:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                lis.append(i.get('href'))\n",
    "        for i in range(0, len(li)-1):\n",
    "            if 'movies' in li[i]:\n",
    "                MasterURLs.append(li[i])\n",
    "        for i in range(0, len(lis)-1):\n",
    "            if 'movies' in lis[i]:\n",
    "                MasterURLs.append(lis[i])\n",
    "    for i in range(0, len(MasterURLs)):\n",
    "        MasterURLs[i] = 'https://www.boxofficemojo.com'+ MasterURLs[i]\n",
    "    return MasterURLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.boxofficemojo.com/movies/?id=leap.htm'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url = 'https://www.boxofficemojo.com/yearly/'\n",
    "\n",
    "# MasterURLs = MasterURLsparser(url)\n",
    "# YearlyTop100sparser = YearlyTop100sparser(url)\n",
    "MasterURLs[149]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "MasterURLBatch332 = MasterURLs[:332]\n",
    "MasterURLBatch1 = MasterURLs[:7200]\n",
    "MasterURLBatch2 = MasterURLs[7200:14400]\n",
    "MasterURLBatch3 = MasterURLs[14400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for url in MasterURLBatch1:\n",
    "    df = df.append(\n",
    "        {\n",
    "            'Title': titleparser(url)\n",
    "#             , 'Distributor': distributorparser(url)\n",
    "#             , 'Genre': genreparser(url)\n",
    "#             , 'MPAA Rating': mpaaratingparser(url)\n",
    "#             , 'Production Budget': productionbudgetparser(url)\n",
    "#             , 'Release Date': releasedateparser(url)\n",
    "#             , 'Runtime': runtimeparser(url)\n",
    "#             , 'Domestic': domesticparser(url)\n",
    "#             , 'Foreign': foreignparser(url)\n",
    "#             , 'Worldwide': worldwideparser(url)\n",
    "#             , 'Opening Weekend Gross': openingweekendgrossparser(url)\n",
    "#             , 'Opening Weekend Theaters': openingweekendtheatersparser(url)\n",
    "#             , 'Widest Theaters': widestreleaseparser(url)\n",
    "#             , 'Director': directorparser(url)\n",
    "#             , 'Writer': writerparser(url)\n",
    "#             , 'Actors': actorparser(url)\n",
    "#             , 'Producer': producerparser(url)\n",
    "#             , 'Composer': composerparser(url)\n",
    "#             , 'Genres': genresparser(url)\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "    print(str(df.index.values[-1]) + ' ' + str(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downloadhtml(url):\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        if 'Sorry, we\\'re not able to process your request.' in BeautifulSoup(page.content, 'lxml').find('div', id='main').find('p'):\n",
    "            return None\n",
    "        else:\n",
    "            soup = BeautifulSoup(page.content, 'lxml')\n",
    "            return soup\n",
    "    except TimeoutError:\n",
    "        try:\n",
    "            page = requests.get(url)\n",
    "            if 'Sorry, we\\'re not able to process your request.' in BeautifulSoup(page.content, 'lxml').find('div', id='main').find('p'):\n",
    "                return None\n",
    "            else:\n",
    "                soup = BeautifulSoup(page.content, 'lxml')\n",
    "                return soup\n",
    "        except TimeoutError:\n",
    "            return 'TimeoutError'\n",
    "\n",
    "def uppertabledataparser(soup):\n",
    "    upper_table_data_rows = soup.find(\n",
    "        'div', id='body').find(\n",
    "        'table', style='padding-top: 5px;').find(\n",
    "        'table', bgcolor='#dcdcdc').find_all(\n",
    "        'tr')\n",
    "    return upper_table_data_rows\n",
    "\n",
    "def distributorparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    if soup is None:\n",
    "        return 'Sorry, we are not able to process your request.'\n",
    "    else:\n",
    "        while True:\n",
    "            try:\n",
    "                upper_table_data_rows = uppertabledataparser(soup)\n",
    "                li = []\n",
    "                for tr in upper_table_data_rows:\n",
    "                    td = tr.find_all('td')\n",
    "                    for i in td:\n",
    "                        li.append(i.text)\n",
    "                upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "                for i in upper_table_dict.keys():\n",
    "                    upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "                return upper_table_dict['Distributor']\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 https://www.boxofficemojo.com/movies/?id=marvel2017b.htm\n",
      "1 https://www.boxofficemojo.com/movies/?id=theincredibles2.htm\n",
      "2 https://www.boxofficemojo.com/movies/?id=foxmarvel18.htm\n",
      "3 https://www.boxofficemojo.com/movies/?id=ant-manandthewasp.htm\n",
      "4 https://www.boxofficemojo.com/movies/?id=aquietplace.htm\n",
      "5 https://www.boxofficemojo.com/movies/?id=crazyrichasians.htm\n",
      "6 https://www.boxofficemojo.com/movies/?id=wbeventfilm2018c.htm\n",
      "7 https://www.boxofficemojo.com/movies/?id=mammamia2.htm\n",
      "8 https://www.boxofficemojo.com/movies/?id=theequalizer2.htm\n",
      "9 https://www.boxofficemojo.com/movies/?id=ladisneyfairytale22018.htm\n",
      "10 https://www.boxofficemojo.com/movies/?id=newlinetentpole2018.htm\n",
      "11 https://www.boxofficemojo.com/movies/?id=icanonlyimagine.htm\n",
      "12 https://www.boxofficemojo.com/movies/?id=gamenight.htm\n",
      "13 https://www.boxofficemojo.com/movies/?id=skyscraper.htm\n",
      "14 https://www.boxofficemojo.com/movies/?id=unirratedcomedy.htm\n",
      "15 https://www.boxofficemojo.com/movies/?id=mazerunner3.htm\n",
      "16 https://www.boxofficemojo.com/movies/?id=tag.htm\n",
      "17 https://www.boxofficemojo.com/movies/?id=overboard2018.htm\n",
      "18 https://www.boxofficemojo.com/movies/?id=ifeelpretty.htm\n",
      "19 https://www.boxofficemojo.com/movies/?id=redsparrow.htm\n",
      "20 https://www.boxofficemojo.com/movies/?id=horsesoldiers2018.htm\n",
      "21 https://www.boxofficemojo.com/movies/?id=a24horrora.htm\n",
      "22 https://www.boxofficemojo.com/movies/?id=sherlockgnomes.htm\n",
      "23 https://www.boxofficemojo.com/movies/?id=thepredator.htm\n",
      "24 https://www.boxofficemojo.com/movies/?id=untitledgregberlantifilm.htm\n",
      "25 https://www.boxofficemojo.com/movies/?id=thecommuter.htm\n",
      "26 https://www.boxofficemojo.com/movies/?id=mile22.htm\n",
      "27 https://www.boxofficemojo.com/movies/?id=deathwish2017.htm\n",
      "28 https://www.boxofficemojo.com/movies/?id=asimplefavor.htm\n",
      "29 https://www.boxofficemojo.com/movies/?id=isleofdogs.htm\n",
      "30 https://www.boxofficemojo.com/movies/?id=supertroopers2.htm\n",
      "31 https://www.boxofficemojo.com/movies/?id=slenderman.htm\n",
      "32 https://www.boxofficemojo.com/movies/?id=thehousewithaclockinitswalls.htm\n",
      "33 https://www.boxofficemojo.com/movies/?id=strangers2.htm\n",
      "34 https://www.boxofficemojo.com/movies/?id=bemyneighbor.htm\n",
      "35 https://www.boxofficemojo.com/movies/?id=thehappytimemurders.htm\n",
      "36 https://www.boxofficemojo.com/movies/?id=showdogs.htm\n",
      "37 https://www.boxofficemojo.com/movies/?id=paulapostleofchrist.htm\n",
      "38 https://www.boxofficemojo.com/movies/?id=chappaquiddick.htm\n",
      "39 https://www.boxofficemojo.com/movies/?id=forevermygirl.htm\n",
      "40 https://www.boxofficemojo.com/movies/?id=eighthgrade.htm\n",
      "41 https://www.boxofficemojo.com/movies/?id=threeidenticalstrangers.htm\n",
      "42 https://www.boxofficemojo.com/movies/?id=padmaavat.htm\n",
      "43 https://www.boxofficemojo.com/movies/?id=midnightsun.htm\n",
      "44 https://www.boxofficemojo.com/movies/?id=traffik.htm\n",
      "45 https://www.boxofficemojo.com/movies/?id=earlyman.htm\n",
      "46 https://www.boxofficemojo.com/movies/?id=sanju.htm\n",
      "47 https://www.boxofficemojo.com/movies/?id=dogdays2018.htm\n",
      "48 https://www.boxofficemojo.com/movies/?id=axl.htm\n",
      "49 https://www.boxofficemojo.com/movies/?id=everyday2018.htm\n",
      "50 https://www.boxofficemojo.com/movies/?id=marvel0518.htm\n",
      "51 https://www.boxofficemojo.com/movies/?id=jurassicworldsequel.htm\n",
      "52 https://www.boxofficemojo.com/movies/?id=missionimpossible6.htm\n",
      "53 https://www.boxofficemojo.com/movies/?id=untitledhansolostarwarsanthologyfilm.htm\n",
      "54 https://www.boxofficemojo.com/movies/?id=hoteltransylvania3.htm\n",
      "55 https://www.boxofficemojo.com/movies/?id=wbeventfilm2018.htm\n",
      "56 https://www.boxofficemojo.com/movies/?id=readyplayerone.htm\n",
      "57 https://www.boxofficemojo.com/movies/?id=peterrabbit.htm\n",
      "58 https://www.boxofficemojo.com/movies/?id=thenun.htm\n",
      "59 https://www.boxofficemojo.com/movies/?id=fiftyshadesfreed.htm\n",
      "60 https://www.boxofficemojo.com/movies/?id=disneyfairytale2017.htm\n",
      "61 https://www.boxofficemojo.com/movies/?id=purge4.htm\n",
      "62 https://www.boxofficemojo.com/movies/?id=bookclub.htm\n",
      "63 https://www.boxofficemojo.com/movies/?id=insidious4.htm\n",
      "64 https://www.boxofficemojo.com/movies/?id=pacificrim2.htm\n",
      "65 https://www.boxofficemojo.com/movies/?id=dcfilm0318.htm\n",
      "66 https://www.boxofficemojo.com/movies/?id=lifeoftheparty.htm\n",
      "67 https://www.boxofficemojo.com/movies/?id=sicario2.htm\n",
      "68 https://www.boxofficemojo.com/movies/?id=blackkklansman.htm\n",
      "69 https://www.boxofficemojo.com/movies/?id=breakingin2018.htm\n",
      "70 https://www.boxofficemojo.com/movies/?id=denofthieves.htm\n",
      "71 https://www.boxofficemojo.com/movies/?id=acrimony.htm\n",
      "72 https://www.boxofficemojo.com/movies/?id=uncledrew.htm\n",
      "73 https://www.boxofficemojo.com/movies/?id=truthordare2017.htm\n",
      "74 https://www.boxofficemojo.com/movies/?id=paddington2.htm\n",
      "75 https://www.boxofficemojo.com/movies/?id=1517toparis.htm\n",
      "76 https://www.boxofficemojo.com/movies/?id=sonyeventfilm2017.htm\n",
      "77 https://www.boxofficemojo.com/movies/?id=thespywhodumpedme.htm\n",
      "78 https://www.boxofficemojo.com/movies/?id=annihilation.htm\n",
      "79 https://www.boxofficemojo.com/movies/?id=adrift.htm\n",
      "80 https://www.boxofficemojo.com/movies/?id=peppermint.htm\n",
      "81 https://www.boxofficemojo.com/movies/?id=wbanimation62018.htm\n",
      "82 https://www.boxofficemojo.com/movies/?id=winchester.htm\n",
      "83 https://www.boxofficemojo.com/movies/?id=searching.htm\n",
      "84 https://www.boxofficemojo.com/movies/?id=proudmary.htm\n",
      "85 https://www.boxofficemojo.com/movies/?id=superfly.htm\n",
      "86 https://www.boxofficemojo.com/movies/?id=whiteboyrick.htm\n",
      "87 https://www.boxofficemojo.com/movies/?id=sorrytobotheryou.htm\n",
      "88 https://www.boxofficemojo.com/movies/?id=operationfinale.htm\n",
      "89 https://www.boxofficemojo.com/movies/?id=rbg.htm\n",
      "90 https://www.boxofficemojo.com/movies/?id=thedarkestminds.htm\n",
      "91 https://www.boxofficemojo.com/movies/?id=upgrade.htm\n",
      "92 https://www.boxofficemojo.com/movies/?id=themiracleseason.htm\n",
      "93 https://www.boxofficemojo.com/movies/?id=tully2018.htm\n",
      "94 https://www.boxofficemojo.com/movies/?id=unfriended2.htm\n",
      "95 https://www.boxofficemojo.com/movies/?id=thedeathofstalin.htm\n",
      "96 https://www.boxofficemojo.com/movies/?id=unsane.htm\n",
      "97 https://www.boxofficemojo.com/movies/?id=hotelartemis.htm\n",
      "98 https://www.boxofficemojo.com/movies/?id=hurricaneheist.htm\n",
      "99 https://www.boxofficemojo.com/movies/?id=leavenotrace.htm\n",
      "100 https://www.boxofficemojo.com/movies/?id=starwars8.htm\n",
      "101 https://www.boxofficemojo.com/movies/?id=wonderwoman.htm\n",
      "102 https://www.boxofficemojo.com/movies/?id=marvel17a.htm\n",
      "103 https://www.boxofficemojo.com/movies/?id=it.htm\n",
      "104 https://www.boxofficemojo.com/movies/?id=despicableme3.htm\n",
      "105 https://www.boxofficemojo.com/movies/?id=wolverine2017.htm\n",
      "106 https://www.boxofficemojo.com/movies/?id=pixar1117.htm\n",
      "107 https://www.boxofficemojo.com/movies/?id=blumhouse2.htm\n",
      "108 https://www.boxofficemojo.com/movies/?id=bossbaby.htm\n",
      "109 https://www.boxofficemojo.com/movies/?id=potc5.htm\n",
      "110 https://www.boxofficemojo.com/movies/?id=cars3.htm\n",
      "111 https://www.boxofficemojo.com/movies/?id=split2017.htm\n",
      "112 https://www.boxofficemojo.com/movies/?id=transformers5.htm\n",
      "113 https://www.boxofficemojo.com/movies/?id=fiftyshadesdarker.htm\n",
      "114 https://www.boxofficemojo.com/movies/?id=pitchperfect3.htm\n",
      "115 https://www.boxofficemojo.com/movies/?id=murderorientexpress17.htm\n",
      "116 https://www.boxofficemojo.com/movies/?id=kingsman2.htm\n",
      "117 https://www.boxofficemojo.com/movies/?id=johnwick2.htm\n",
      "118 https://www.boxofficemojo.com/movies/?id=powerrangers16.htm\n",
      "119 https://www.boxofficemojo.com/movies/?id=untitledstevenspielberg.htm\n",
      "120 https://www.boxofficemojo.com/movies/?id=hitmansbodyguard.htm\n",
      "121 https://www.boxofficemojo.com/movies/?id=captainunderpants.htm\n",
      "122 https://www.boxofficemojo.com/movies/?id=adogspurpose.htm\n",
      "123 https://www.boxofficemojo.com/movies/?id=ninjago.htm\n",
      "124 https://www.boxofficemojo.com/movies/?id=theshack.htm\n",
      "125 https://www.boxofficemojo.com/movies/?id=blumhousehorror2018.htm\n",
      "126 https://www.boxofficemojo.com/movies/?id=thecoldestcity.htm\n",
      "127 https://www.boxofficemojo.com/movies/?id=darktower.htm\n",
      "128 https://www.boxofficemojo.com/movies/?id=amadeahalloween2.htm\n",
      "129 https://www.boxofficemojo.com/movies/?id=greatwall.htm\n",
      "130 https://www.boxofficemojo.com/movies/?id=goinginsty2017.htm\n",
      "131 https://www.boxofficemojo.com/movies/?id=xxx3.htm\n",
      "132 https://www.boxofficemojo.com/movies/?id=thebigsick.htm\n",
      "133 https://www.boxofficemojo.com/movies/?id=thelamb.htm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134 https://www.boxofficemojo.com/movies/?id=kingarthur2016.htm\n",
      "135 https://www.boxofficemojo.com/movies/?id=americanassassin.htm\n",
      "136 https://www.boxofficemojo.com/movies/?id=everythingeverything.htm\n",
      "137 https://www.boxofficemojo.com/movies/?id=geostorm.htm\n",
      "138 https://www.boxofficemojo.com/movies/?id=fistfight.htm\n",
      "139 https://www.boxofficemojo.com/movies/?id=kidnap2015.htm\n",
      "140 https://www.boxofficemojo.com/movies/?id=mountainbetweenus.htm\n",
      "141 https://www.boxofficemojo.com/movies/?id=itonya.htm\n",
      "142 https://www.boxofficemojo.com/movies/?id=mollysgame.htm\n",
      "143 https://www.boxofficemojo.com/movies/?id=rings.htm\n",
      "144 https://www.boxofficemojo.com/movies/?id=homeagain.htm\n",
      "145 https://www.boxofficemojo.com/movies/?id=thehouse.htm\n",
      "146 https://www.boxofficemojo.com/movies/?id=gifted.htm\n",
      "147 https://www.boxofficemojo.com/movies/?id=thebyebyeman.htm\n",
      "148 https://www.boxofficemojo.com/movies/?id=rockthatbody.htm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-898ed6b1ba08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     df = df.append(\n\u001b[1;32m      5\u001b[0m         {\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0;34m'Distributor'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdistributorparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         }, ignore_index=True\n\u001b[1;32m      8\u001b[0m     )\n",
      "\u001b[0;32m<ipython-input-189-730c6f134a39>\u001b[0m in \u001b[0;36mdistributorparser\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mupper_table_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-730c6f134a39>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mupper_table_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "\n",
    "for url in MasterURLBatch332:\n",
    "    df = df.append(\n",
    "        {\n",
    "            'Distributor': distributorparser(url)\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "    print(str(df.index.values[-1]) + ' ' + str(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAttributeError = df[df['Title'] == 'AttributeError']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Title]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    print(df)\n",
    "# When df is 800, then I get 321, 323, & 327 as AttributeErrors. With a total of 464/800.\n",
    "# When df is 800, then I get 321, 323, & 327 as AttributeErrors. With a total of 464/800."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-0216326957c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistributorparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://www.boxofficemojo.com/movies/?id=leap.htm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-189-730c6f134a39>\u001b[0m in \u001b[0;36mdistributorparser\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mupper_table_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-189-730c6f134a39>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     39\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                         \u001b[0mli\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mupper_table_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupper_table_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "print(distributorparser('https://www.boxofficemojo.com/movies/?id=leap.htm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
