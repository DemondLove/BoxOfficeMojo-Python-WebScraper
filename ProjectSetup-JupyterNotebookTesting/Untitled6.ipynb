{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# BoxOfficeMojo Python WebScraper Utility Package\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def downloadhtml(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    return soup\n",
    "\n",
    "\n",
    "def uppertabledataparser(soup):\n",
    "    upper_table_data_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find('table', bgcolor='#dcdcdc').find_all('tr')\n",
    "    return upper_table_data_rows\n",
    "\n",
    "\n",
    "def totallifetimegrossestableparser(soup):\n",
    "    total_lifetime_grosses_table = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('table').find_all('td')\n",
    "    return total_lifetime_grosses_table\n",
    "\n",
    "\n",
    "def titleparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    title = soup.find('div', id='body').find('table', style='padding-top: 5px;').find('b').text\n",
    "    return title\n",
    "\n",
    "\n",
    "def distributorparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    upper_table_data_rows = uppertabledataparser(soup)\n",
    "    li = []\n",
    "    for tr in upper_table_data_rows:\n",
    "        td = tr.find_all('td')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "    for i in upper_table_dict.keys():\n",
    "        upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "    return upper_table_dict['Distributor']\n",
    "\n",
    "\n",
    "def domestictotalgrossparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    upper_table_data_rows = uppertabledataparser(soup)\n",
    "    li = []\n",
    "    for tr in upper_table_data_rows:\n",
    "        td = tr.find_all('td')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "    for i in upper_table_dict.keys():\n",
    "        upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "    return upper_table_dict['Domestic Total Gross']\n",
    "\n",
    "\n",
    "def genreparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    upper_table_data_rows = uppertabledataparser(soup)\n",
    "    li = []\n",
    "    for tr in upper_table_data_rows:\n",
    "        td = tr.find_all('td')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "    for i in upper_table_dict.keys():\n",
    "        upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "    return upper_table_dict['Genre']\n",
    "\n",
    "\n",
    "def mpaaratingparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    upper_table_data_rows = uppertabledataparser(soup)\n",
    "    li = []\n",
    "    for tr in upper_table_data_rows:\n",
    "        td = tr.find_all('td')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "    for i in upper_table_dict.keys():\n",
    "        upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "    return upper_table_dict['MPAA Rating']\n",
    "\n",
    "\n",
    "def productionbudgetparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    upper_table_data_rows = uppertabledataparser(soup)\n",
    "    li = []\n",
    "    for tr in upper_table_data_rows:\n",
    "        td = tr.find_all('td')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "    for i in upper_table_dict.keys():\n",
    "        upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "    return upper_table_dict['Production Budget']\n",
    "\n",
    "\n",
    "def releasedateparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    upper_table_data_rows = uppertabledataparser(soup)\n",
    "    li = []\n",
    "    for tr in upper_table_data_rows:\n",
    "        td = tr.find_all('td')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "    for i in upper_table_dict.keys():\n",
    "        upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "    return upper_table_dict['Release Date']\n",
    "\n",
    "\n",
    "def runtimeparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    upper_table_data_rows = uppertabledataparser(soup)\n",
    "    li = []\n",
    "    for tr in upper_table_data_rows:\n",
    "        td = tr.find_all('td')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    upper_table_dict = {k:v for k,v in (x.split(':') for x in li)}\n",
    "    for i in upper_table_dict.keys():\n",
    "        upper_table_dict[i] = upper_table_dict[i].lstrip()\n",
    "    return upper_table_dict['Runtime']\n",
    "\n",
    "\n",
    "def domesticparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    total_lifetime_grosses_table_rows = totallifetimegrossestableparser(soup)\n",
    "    li = []\n",
    "    for tr in total_lifetime_grosses_table_rows:\n",
    "        li.append(tr.text)\n",
    "    for x in range(0, len(li)):\n",
    "        li[x] = li[x].replace('\\xa0', '')\n",
    "        li[x] = li[x].replace('+', '')\n",
    "        li[x] = li[x].replace('=', '')\n",
    "        li[x] = li[x].replace(':', '')\n",
    "    li = [x for x in li if x]\n",
    "    return li[1]\n",
    "\n",
    "\n",
    "def foreignparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    total_lifetime_grosses_table_rows = totallifetimegrossestableparser(soup)\n",
    "    li = []\n",
    "    for tr in total_lifetime_grosses_table_rows:\n",
    "        li.append(tr.text)\n",
    "    for x in range(0, len(li)):\n",
    "        li[x] = li[x].replace('\\xa0', '')\n",
    "        li[x] = li[x].replace('+', '')\n",
    "        li[x] = li[x].replace('=', '')\n",
    "        li[x] = li[x].replace(':', '')\n",
    "    li = [x for x in li if x]\n",
    "    return li[4]\n",
    "\n",
    "\n",
    "def worldwideparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    total_lifetime_grosses_table_rows = totallifetimegrossestableparser(soup)\n",
    "    li = []\n",
    "    for tr in total_lifetime_grosses_table_rows:\n",
    "        li.append(tr.text)\n",
    "    for x in range(0, len(li)):\n",
    "        li[x] = li[x].replace('\\xa0', '')\n",
    "        li[x] = li[x].replace('+', '')\n",
    "        li[x] = li[x].replace('=', '')\n",
    "        li[x] = li[x].replace(':', '')\n",
    "    li = [x for x in li if x]\n",
    "    return li[7]\n",
    "\n",
    "\n",
    "def openingweekendgrossparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    domesticsummary_table_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('table').find_next('table').find_all('td')\n",
    "    li = []\n",
    "    for tr in domesticsummary_table_rows:\n",
    "        li.append(tr.text)\n",
    "    for x in range(0, len(li)):\n",
    "        li[x] = li[x].replace('\\xa0', '')\n",
    "        li[x] = li[x].replace('+', '')\n",
    "        li[x] = li[x].replace('=', '')\n",
    "        li[x] = li[x].replace(':', '')\n",
    "    return li[1]\n",
    "\n",
    "\n",
    "def openingweekendtheatersparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    domesticsummary_table_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('table').find_next('table').find_all('td')\n",
    "    li = []\n",
    "    for tr in domesticsummary_table_rows:\n",
    "        li.append(tr.text)\n",
    "    for x in range(0, len(li)):\n",
    "        li[x] = li[x].replace('\\xa0', '')\n",
    "        li[x] = li[x].replace('+', '')\n",
    "        li[x] = li[x].replace('=', '')\n",
    "        li[x] = li[x].replace(':', '')\n",
    "    k = li[2].split(', ')\n",
    "    return k[1]\n",
    "\n",
    "\n",
    "def widestreleaseparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    widest_release_table_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('table').find_next('table').find_next('table').find_all('tr')\n",
    "    li = []\n",
    "    for tr in widest_release_table_rows:\n",
    "        td = tr.find_all('td')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    for x in range(0, len(li)):\n",
    "        li[x] = li[x].replace('\\xa0', ' ')\n",
    "        li[x] = li[x].lstrip()\n",
    "        li[x] = li[x].replace(':', '')\n",
    "    return li[1]\n",
    "\n",
    "\n",
    "def directorparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "    li = []\n",
    "    for tr in the_players_rows:\n",
    "        td = tr.find_all('a')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    for i in range(0, len(li)-1):\n",
    "        if '*' in li[i]:\n",
    "            del li[i]\n",
    "    for i in range(0, len(li)-1):\n",
    "        if 'Director' in li[i]:\n",
    "            director_index = i\n",
    "        elif 'Writer' in li[i]:\n",
    "            writer_index = i\n",
    "    director_data = li[director_index+1:writer_index]\n",
    "    if len(li[director_index+1:writer_index]) == 1:\n",
    "        director_data = li[director_index+1]\n",
    "    return str(director_data)\n",
    "\n",
    "\n",
    "def writerparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "    li = []\n",
    "    for tr in the_players_rows:\n",
    "        td = tr.find_all('a')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    for i in range(0, len(li)-1):\n",
    "        if '*' in li[i]:\n",
    "            del li[i]\n",
    "    for i in range(0, len(li)-1):\n",
    "        if 'Writer' in li[i]:\n",
    "            writer_index = i\n",
    "        elif 'Actors' in li[i]:\n",
    "            actor_index = i\n",
    "    writer_data = li[writer_index+1:actor_index]\n",
    "    if len(li[writer_index+1:actor_index]) == 1:\n",
    "        writer_data = li[writer_index+1]\n",
    "    return str(writer_data)\n",
    "\n",
    "\n",
    "def actorparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "    li = []\n",
    "    for tr in the_players_rows:\n",
    "        td = tr.find_all('a')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    for i in range(0, len(li)-1):\n",
    "        if '*' in li[i]:\n",
    "            del li[i]\n",
    "    for i in range(0, len(li)-1):\n",
    "        if 'Actors' in li[i]:\n",
    "            actor_index = i\n",
    "        elif 'Producer' in li[i]:\n",
    "            producer_index = i\n",
    "    actors_data = li[actor_index+1:producer_index]\n",
    "    if len(li[actor_index+1:producer_index]) == 1:\n",
    "        actors_data = li[actor_index+1]\n",
    "    return str(actors_data)\n",
    "\n",
    "\n",
    "def producerparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "    li = []\n",
    "    for tr in the_players_rows:\n",
    "        td = tr.find_all('a')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    for i in range(0, len(li)-1):\n",
    "        if '*' in li[i]:\n",
    "            del li[i]\n",
    "    for i in range(0, len(li)-1):\n",
    "        if 'Producer' in li[i]:\n",
    "            producer_index = i\n",
    "        elif 'Composer' in li[i]:\n",
    "            composer_index = i\n",
    "    producer_data = li[producer_index+1:composer_index]\n",
    "    if len(li[producer_index+1:composer_index]) == 1:\n",
    "        producer_data = li[producer_index+1]\n",
    "    return str(producer_data)\n",
    "\n",
    "\n",
    "def composerparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "    li = []\n",
    "    for tr in the_players_rows:\n",
    "        td = tr.find_all('a')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    for i in range(0, len(li)-1):\n",
    "        if '*' in li[i]:\n",
    "            del li[i]\n",
    "    for i in range(0, len(li)-1):\n",
    "        if 'Composer' in li[i]:\n",
    "            composer_index = i\n",
    "    composer_data = li[composer_index+1:]\n",
    "    if len(composer_data) == 1:\n",
    "        composer_data = li[composer_index+1]\n",
    "    return str(composer_data)\n",
    "\n",
    "\n",
    "def genresparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    genres_table_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find_next_sibling('div').find_all('a')\n",
    "    li = []\n",
    "    for tr in genres_table_rows:\n",
    "        td = tr.find_all('b')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    return str(li)\n",
    "\n",
    "\n",
    "def YearlyTop100sparser(url):\n",
    "    page = requests.get(url)\n",
    "    year = BeautifulSoup(page.content, 'lxml').find('div', id='body').find('table').find_next('table').find_next('table')\n",
    "    li = []\n",
    "    td = year.find_all('a')\n",
    "    for i in td:\n",
    "        li.append(i.get('href'))\n",
    "    YearlyTop100s = []\n",
    "    for i in range(0, len(li)-1):\n",
    "        if 'chart' in li[i]:\n",
    "            YearlyTop100s.append(li[i])\n",
    "    for i in range(0, len(YearlyTop100s)):\n",
    "        YearlyTop100s[i] = 'https://www.boxofficemojo.com/yearly/'+ YearlyTop100s[i]\n",
    "    return YearlyTop100s\n",
    "\n",
    "\n",
    "def YearlyNonTop100sparser(url):\n",
    "    YearlyTop100s = YearlyTop100sparser(url)\n",
    "    YearlyNonTop100s = []\n",
    "    for x in YearlyTop100s:\n",
    "        page = requests.get(x)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        footer = soup.find('center')\n",
    "        footer_url_rows = footer.find_all('a')\n",
    "        for i in footer_url_rows:\n",
    "            YearlyNonTop100s.append(i.get('href'))\n",
    "    for i in range(0, len(YearlyNonTop100s)):\n",
    "        YearlyNonTop100s[i] = 'https://www.boxofficemojo.com'+ YearlyNonTop100s[i]\n",
    "    return YearlyNonTop100s\n",
    "\n",
    "\n",
    "def AllYearlyURLsparser(url):\n",
    "    AllYearlyURLs = []\n",
    "    YearlyTop100s = YearlyTop100sparser(url)\n",
    "    YearlyNonTop100s = YearlyNonTop100sparser(url)\n",
    "    for x in YearlyTop100s:\n",
    "        AllYearlyURLs.append(x)\n",
    "    for x in YearlyNonTop100s:\n",
    "        AllYearlyURLs.append(x)\n",
    "    return AllYearlyURLs\n",
    "\n",
    "\n",
    "def MasterURLsparser(url):\n",
    "    MasterURLs = []\n",
    "    AllYearlyURLs = AllYearlyURLsparser(url)\n",
    "    for x in AllYearlyURLs:\n",
    "        page = requests.get(x)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        body = soup.find('table', cellpadding='5')\n",
    "        urls_rows_ffffff = body.find_all('tr', bgcolor='#ffffff')\n",
    "        li = []\n",
    "        for tr in urls_rows_ffffff:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                li.append(i.get('href'))\n",
    "        urls_rows_f4f4ff = body.find_all('tr', bgcolor='#f4f4ff')\n",
    "        lis = []\n",
    "        for tr in urls_rows_f4f4ff:\n",
    "            td = tr.find_all('a')\n",
    "            for i in td:\n",
    "                lis.append(i.get('href'))\n",
    "        for i in range(0, len(li)-1):\n",
    "            if 'movies' in li[i]:\n",
    "                MasterURLs.append(li[i])\n",
    "        for i in range(0, len(lis)-1):\n",
    "            if 'movies' in lis[i]:\n",
    "                MasterURLs.append(lis[i])\n",
    "    for i in range(0, len(MasterURLs)):\n",
    "        MasterURLs[i] = 'https://www.boxofficemojo.com'+ MasterURLs[i]\n",
    "    return MasterURLs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.boxofficemojo.com/yearly/'\n",
    "\n",
    "# MasterURLs = MasterURLsparser(url)\n",
    "\n",
    "# df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directorparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "    li = []\n",
    "    for tr in the_players_rows:\n",
    "        td = tr.find_all('a')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    for i in range(0, len(li)-1):\n",
    "        if '*' in li[i]:\n",
    "            del li[i]\n",
    "    for i in range(0, len(li)-1):\n",
    "        if 'Director' in li[i]:\n",
    "            director_index = i\n",
    "        elif 'Writer' in li[i]:\n",
    "            writer_index = i    \n",
    "    try:\n",
    "        director_index\n",
    "    except NameError:\n",
    "        director_index = None\n",
    "    \n",
    "    try:\n",
    "        writer_index\n",
    "    except NameError:\n",
    "        writer_index = None\n",
    "    \n",
    "    if director_index is None:\n",
    "        director_data = None\n",
    "    elif writer_index is None:\n",
    "        director_data = li[1]\n",
    "    elif len(li[director_index+1:writer_index]) == 1:\n",
    "        director_data = li[director_index+1]\n",
    "    else:\n",
    "        director_data = li[director_index+1:writer_index]\n",
    "        \n",
    "    return str(director_data)\n",
    "\n",
    "def writerparser(url):\n",
    "    soup = downloadhtml(url)\n",
    "    the_players_rows = soup.find('div', id='body').find('table', style='padding-top: 5px;').find_next_sibling('table').find('table', width='100%').find('table', width='100%').find('td', style='padding-left: 10px;').find('table').find_all('tr')\n",
    "    li = []\n",
    "    for tr in the_players_rows:\n",
    "        td = tr.find_all('a')\n",
    "        for i in td:\n",
    "            li.append(i.text)\n",
    "    for i in range(0, len(li)-1):\n",
    "        if '*' in li[i]:\n",
    "            del li[i]\n",
    "    for i in range(0, len(li)-1):\n",
    "        if 'Writer' in li[i]:\n",
    "            writer_index = i\n",
    "        elif 'Actors' in li[i]:\n",
    "            actor_index = i\n",
    "\n",
    "    try:\n",
    "        writer_index\n",
    "    except NameError:\n",
    "        writer_index = None\n",
    "\n",
    "    try:\n",
    "        actor_index\n",
    "    except NameError:\n",
    "        actor_index = None\n",
    "\n",
    "    if writer_index is None:\n",
    "        writer_data = None\n",
    "    elif actor_index is None:\n",
    "        writer_data = li[4]\n",
    "    elif len(li[writer_index+1:actor_index]) == 1:\n",
    "        writer_data = li[writer_index+1]\n",
    "    else:\n",
    "        writer_data = li[writer_index+1:actor_index]\n",
    "    return str(writer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.boxofficemojo.com/movies/?id=icanonlyimagine.htm'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MasterURLs[11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "MasterURL = MasterURLs[:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.boxofficemojo.com/movies/?id=marvel2017b.htm\n",
      "https://www.boxofficemojo.com/movies/?id=theincredibles2.htm\n",
      "https://www.boxofficemojo.com/movies/?id=foxmarvel18.htm\n",
      "https://www.boxofficemojo.com/movies/?id=ant-manandthewasp.htm\n",
      "https://www.boxofficemojo.com/movies/?id=aquietplace.htm\n",
      "https://www.boxofficemojo.com/movies/?id=crazyrichasians.htm\n",
      "https://www.boxofficemojo.com/movies/?id=wbeventfilm2018c.htm\n",
      "https://www.boxofficemojo.com/movies/?id=mammamia2.htm\n",
      "https://www.boxofficemojo.com/movies/?id=theequalizer2.htm\n",
      "https://www.boxofficemojo.com/movies/?id=ladisneyfairytale22018.htm\n",
      "https://www.boxofficemojo.com/movies/?id=newlinetentpole2018.htm\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-05121be52523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0;34m'Runtime'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdomesticparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0;34m'Domestic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdomesticparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0;34m,\u001b[0m \u001b[0;34m'Foreign'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mforeignparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0;34m'Worldwide'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mworldwideparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;34m,\u001b[0m \u001b[0;34m'Opening Weekend Gross'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mopeningweekendgrossparser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-555310cc5dee>\u001b[0m in \u001b[0;36mforeignparser\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0mli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for url in MasterURLs:\n",
    "    df = df.append(\n",
    "        {\n",
    "            'Title': titleparser(url)\n",
    "            , 'Distributor': distributorparser(url)\n",
    "            , 'Genre': genreparser(url)\n",
    "            , 'MPAA Rating': mpaaratingparser(url)\n",
    "            , 'Production Budget': productionbudgetparser(url)\n",
    "            , 'Release Date': releasedateparser(url)\n",
    "            , 'Runtime': runtimeparser(url)\n",
    "            , 'Domestic': domesticparser(url)\n",
    "            , 'Foreign': foreignparser(url)\n",
    "            , 'Worldwide': worldwideparser(url)\n",
    "            , 'Opening Weekend Gross': openingweekendgrossparser(url)\n",
    "            , 'Opening Weekend Theaters': openingweekendtheatersparser(url)\n",
    "            , 'Widest Theaters': widestreleaseparser(url)\n",
    "            , 'Director': None #directorparser(url)\n",
    "            , 'Writer': None #writerparser(url)\n",
    "            , 'Actors': None #actorparser(url)\n",
    "            , 'Producer': None #producerparser(url)\n",
    "            , 'Composer': None #composerparser(url)\n",
    "            , 'Genres': genresparser(url)\n",
    "        }, ignore_index=True\n",
    "    )\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Domestic', '$83,482,352']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
